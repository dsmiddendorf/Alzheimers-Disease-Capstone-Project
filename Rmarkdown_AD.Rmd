---
title: "Alzheimer's Disease and its Biopsychosocial Determinants"
author: "Daniel Middendorf"
date: "1/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
##########################################################
# Create alzheimer set, validation set (final hold-out test set)
##########################################################

# Downloading required libraries for analysis if they have not been downloaded yet
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret")
if(!require(readr)) install.packages("readr")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(psych)) install.packages("psych")
if(!require(ggthemes)) install.packages("ggthemes")
if(!require(cowplot)) install.packages("cowplot")
if(!require(corrplot)) install.packages("corrplot")
if(!require(glmnet)) install.packages("glmnet")
if(!require(knitr)) install.packages("knitr")

# Loading libraries used for the following analysis
library(tidyverse)
library(caret)
library(readr)
library(ggplot2)
library(psych)
library(ggthemes)
library(cowplot)
library(corrplot)
library(glmnet)
library(knitr)

# Loading data 
dat <- read_csv("https://raw.githubusercontent.com/dsmiddendorf/Alzheimers-Disease-Capstone-Project/main/oasis_longitudinal.csv")
dat[,c("Hand")] <- NULL # Removing column including handedness as all participants were dextrals

# Change variable name for sex from "M/F" to "Sex"
names(dat)[6] <- "Sex" 

# Validation set will be 10% of the original dataset
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = dat$Group, times = 1, p = 0.1, list = FALSE)
alzheimer <- dat[-test_index,]
validation <- dat[test_index,]
rm(test_index, dat) # Removing original dataset and test index
```

# 1. Introduction
Dementia is the general intellectual deterioration resulting form brain alterations caused by pathology or normal aging (Whalley & Breitner, 2009). However, against popular belief, excessive forms of dementia are not part of the normal aging process and should be evaluated neuropsychologically. Neurodegenerative diseases, including Alzheimer's Disease (AD), are the most prevalent cause of such symptoms and usually occur in old age. AD alone accounts for 60-80% of all dementia cases (Prince, 2007). Despite the prevalence of such diseases, no cure has been developed yet and neuropharmacological drugs can only alliviate the symptoms but not the causes (Scuteri, 2019). As the etiology of such diseases remained unclear, the past thirty years have seen increasingly rapid advances in the field of disease correlates and predictors. The most frequently reported correlates and predictors include: Gender, Age, Education, Socioeconomic Status, and neurophysiological brain correlates (e.g., normalized whole brain volume (nWBV), eTIV (estimated intercranial volume), atlas scaling factor (ASF)). Up to now, far too little attention has been paid to the combined effects of those predictors/correlates. Hence, this paper attempts to find a well-fitting model combining the named risk factors to determine which patients might suffer from dementia or not. Prior to this project, patients underwent a Clinical Dementia Rating by which they were classified as either "demented" or "nondemented" and further underwent extensive medical and neuropsychological examinations. Most patients were investigated more than once; however, in regard to the aim of this project this was not taken into further account in the following analyses. 


## 1.1 Description of the Dataset
The dataset that will be used for our prediction includes 335 rows and 14 columns. Each row represents a examination point of an individual patient. The scale for CDR, our dependent variable, ranges from 0 to 3 with 0 indicating no dementia and 3 indicating severe cognitive decline. The following table gives a description of the column names used in the original dataset.

```{r message=FALSE, , echo=F}
df <- data.frame(Name = c("Sex", "Age", "EDUC", "SES", "MMSE", "CDR", "eTIV", "nWBV", "ASF"),
                 Description = c("Gender", "Age", "Education", "Socioeconomic Status", "Mini Mental Status Examination", "Clinical Dementia Rating",
                                 "Total Intercranial Volume", "Normalized Whole Brain Volume", "Atlas Scaling Factor"))

kable(df)
```


### Clinical Dementia Rating (CDR)
>"Clinical Dementia Rating(CDR) ranges between 0 and 3 which
depends on the issues and leisure activities of the Alzheimer’s
patient. CDR is attributed with having the option to recognize
gentle hindrances, however its shortcomings incorporate the
measure of time it takes to control, its definitive dependence
on emotional evaluation, and relative powerlessness to catch
changes over time." (Kishore et al., 2020)

```{r , echo = F, message = F}
df <- data.frame(Name = c("0", "0.5", "1", "2", "3"),
                 Description = c("None", "Very Mild", "Mild", "Moderate", "Severe"))

kable(df)
```


### Mini Mental Status Examination (MMSE)
> "The Mini-Mental State Examination (MMSE) or Folstein test is a
30-point poll that is utilized broadly in clinical and inquire about
settings to quantify psychological impairment. It is usually utilized
in medication and unified wellbeing to screen for dementia. It is
likewise used to gauge the seriousness and movement of psychological
impedance and to follow the course of subjective changes
in a person after some time; in this way making it a successful
method to record a person’s reaction to treatment." (Kishore et al., 2020)

Although this measure is useful in assessing dementia, we decided to drop this variable at the expense of CDR from the following analysis as CDR involves more involvement of the diagnosing expert. Furthermore, CDR provides us with a clear cut of between "demented" and "nondemented" whereas the MMSE assesses cognitive ability on a nearly complete dimensional model.

### Total Intercranial Volume (eTIV)

> "Estimated Total Intracranial Volume (eTIV) gives the volume of
void spaces in the brain. In the event that eTIV isn’t one-sided, its
difference ought to be irregular past that clarified by the genuine
intracranial volume." (Kishore et al., 2020)

### Normalized Whole Brain Volume (nWBV)
> "Normalize Whole Brain Volume(nWBV) gives the volume of
brain other than void spaces." (Kishore et al., 2020)

### Atlas Scaling Factor (ASF)
> "Atlas Scaling Factor(ASF) is directly proportional to the eTIV and
nWBV. Atlas Scaling Factor characterized as the volume-scaling
factor required to coordinate every person to the chart book target.
Since chart book standardization likens head size, the ASF ought to
be relative to TIV." (Kishore et al., 2020)

## 1.2 Descriptive Statistics

Following you can see the first six rows of the dataset used to predict dementia:

```{r , echo = F}
head(alzheimer)
```

In the following table you can see the general summary statistics of the dataset. For all the variables included in the later model creation, no skew/kurtosis beyond 1 was detected. For some models, such a strong skew might bias results. However, no correction is needed in our case.
```{r , echo = F}
describe(alzheimer[,2:9])
```

Following you can see the gender distribution in percent. The sample has a relatively balanced distribution, indicating that the primary researchers have used a relatively representable sample. 
```{r , echo = F}
table(alzheimer$Sex)/nrow(alzheimer)
```

Following, you can see the number of unique participants included in this study. As the data includes longitudinal records, not every row represents an unique participant. On average each participant was tested 2.248 times.
```{r , echo = F}
length(unique(alzheimer$`Subject ID`))
```

## 1.3 Visualization of Descriptive Statistics

### 1.3.1 Density Diagrams
In the figure below you can see the density diagrams of the continuous data in the dataset that will be used for our statistical models. You can see that the relevant variables are approximately normally distributed. Only MMSE is not; however, we do not include this variable in the following analysis anyway. If the data is strongly skewed, this could pose a problem to some models.

```{r Density Diagramms, echo = F, fig.align='center', comment = "", message = F, results='hide',fig.keep='all'}
# Create Density Diagrams for continuous variables

## Density Plot for Age
dage <- alzheimer %>% ggplot(aes(Age)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

## Density Plot for Education
deduc <- alzheimer %>% ggplot(aes(EDUC)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Density Plot for MMSE (Mini Mental Status Examination)
dmmse <- alzheimer %>% ggplot(aes(MMSE)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Density Plot for eTIV
detiv <- alzheimer %>% ggplot(aes(eTIV)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Density Plot for nWBV
dnwbv <- alzheimer %>% ggplot(aes(nWBV)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Density Plot for ASF
dasf <- alzheimer %>% ggplot(aes(ASF)) + geom_density() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

# Putting all Density Plots together in one Figure
density_plots <- plot_grid(dage, deduc, dmmse, detiv, dnwbv, dasf)
density_plots
```

### 1.3.2 Bar Plots

Below you can see the bar plots of the categorical or ordinal data. You can see that most of the participants were female and most participants did not suffer from dementia. The data also includes participants form a wide range of social classes, which can be seen upon closer inspection of the SES distribution. No participant with a CDR scoring above two was included in this dataset. 

```{r Bar Plots, echo = F, fig.align='center', comment = "", message = F, results='hide',fig.keep='all', warning=F}
# Create barplots for categorical/ordinal data

## Barplot for Sex
x <- alzheimer %>% ggplot(aes(Sex)) + geom_bar() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Barplot for Socioeconomic Status
y <- alzheimer %>% ggplot(aes(SES)) + geom_bar() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

## Barplot for CDR
z <- alzheimer %>% ggplot(aes(CDR)) + geom_bar() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())
plot_grid(x,y,z)
rm(x,y,z)
```

## 1.4 Bivariate Examination of the Data

Following you can see a simple correlation plot between the the continuous variables in this dataset. On the top half you can see the p-values associated with the correlations and on the bottom you can see the real Pearson correlations. We can observe significant associations between CDR and Education and nWBV. This is in line with previous research identifying Education as a protection factor and nWBV as a brain correlate to neurodegenerative diseases like dementia (Cohen, 1994). Nevertheless, such a correlation plot only gives us an idea about some important variables as CDR cannot be seen as a completely continuous variable. We conducted further analyses to find the most influencial variables in determining dementia.

```{r Correlation Plot, echo = F, fig.align='center', comment = "", message = F, results='hide',fig.keep='last', warning=F}
pval <- psych::corr.test(alzheimer[,c(5,7,8,11,12,13,14)], adjust="none")$p # Calculating p-values for the different correlations
pval[pval >= .05] <- NA # Removing p-values above .05
corrplot(cor(alzheimer[,c(5,7,8,11,12,13,14)]), type="upper", p.mat=pval, insig="p-value", 
         tl.pos="n", sig.level=0) # Plotting the upper half of the correlation matrix with p-values

corrplot(cor(alzheimer[,c(5,7,8,11,12,13,14)]), type="lower", add=T, tl.pos="d", cl.pos="n", addCoef.col = T) # Plotting the lower part with correlations
```

Further we investigated the relation between CDR and and the categorical/ordinal variables Sex and SES. Both variables seem dependent on each other with a p-value below .01. As these factors have also been highlighted in the recent literature to be influencial, we included them into our theory-driven model (Chen et al., 2014).
```{r Chi Square Test for Categorical/Ordinal Data, warning = F, echo = F}
chisq.test(alzheimer$Sex, alzheimer$CDR)
chisq.test(alzheimer$Sex, alzheimer$SES)
```

Finally, as we were unable to find a description of the column Group; we thought that this column might be the collapsed CDR variable. Group includes the levels "converted", "demented", and "nondemented." As you can see, only two observations are not fitting if a CDR of 0 is characterized as "nondemented" and all other scorings are characterized as "demented." The category "converted" rightfully includes participants who were at first "nondemented" and converted to "demented." We conclude that the column Group and CDR are the same with two exceptions. These exceptions cannot be explained without having gathered the data ourselves. Following you can also see data from the two participants who do not fit into the classification schema. 

```{r CDR and Group Column the Same, echo = F, fig.align='center', comment = "", message = F,fig.keep='all', warning=F}
# Are the variables CDR and Group the same?
## Creating a barplot of Group ('converted', 'demented', 'Nondemented') filled by CDR
alzheimer %>% ggplot(aes(Group, fill = as.factor(CDR))) + geom_bar() + theme_bw() +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())
## Selecting two participants who would not fit if both variables would be the same
alzheimer[which(alzheimer$CDR == .5 & alzheimer$Group == "Nondemented"),] 
# -> It seems like CDR and Group are the same variable; group is just a collapsed form
# Nevertheless, there are two entries that do not fit. 
```


# 2. Methods and Analysis

# 2.1 Methods
As the data includes missing values in the SES and MMSE column and because the dataset is relatively small, those values were imputed by the median. 

```{r Missing Values, echo = F, fig.align='center', comment = "", results = F, message = F,fig.keep='all', warning=F}
# Replace missing values (NA) with median values
alzheimer$SES[is.na(alzheimer$SES)] <- median(alzheimer$SES, na.rm = T)
alzheimer$MMSE[is.na(alzheimer$MMSE)] <- median(alzheimer$MMSE, na.rm = T)
validation$SES[is.na(validation$SES)] <- median(validation$SES, na.rm = T)
validation$MMSE[is.na(validation$MMSE)] <- median(validation$MMSE, na.rm = T)
```

In order to determine whether participants might suffer from dementia or not, we collapsed the CDR variable into a binary variable: "0", nondemented and "1", demented. This variable is called collapsed CDR (cCDR). When participants had a CDR scoring above 0, they were classified as demented. Otherwise, they were classified as nondemented.

```{r Collapsing CDR, echo = F, fig.align='center', comment = "", results = F, message = F,fig.keep='all', warning=F}
# Collapsing the CDR variable into 0, Nondemented and 1 Demented, new variable is collapsed CDR (cCDR)
alzheimer$cCDR <- ifelse(alzheimer$CDR %in% c(0.5,1,2), 1, 0)
alzheimer$cCDR <- as.factor(alzheimer$cCDR)
alzheimer$CDR <- as.factor(alzheimer$CDR)

validation$cCDR <- ifelse(validation$CDR %in% c(0.5,1,2), 1, 0)
validation$cCDR <- as.factor(validation$cCDR)
validation$CDR <- as.factor(validation$CDR)
```

To identify possible threats of Multicollinearity, we employed a correlation cutoff of .75. From this two variable are dropped from further analyses (ASF and MR Delay). 

```{r Multicollinearity, echo = F, fig.align='center', comment = "", message = F, warning=F}
numeric_variables <- alzheimer[, sapply(alzheimer, is.numeric)]
correlations<-cor(numeric_variables)
highCorr<-findCorrelation(correlations, cutoff = .75)
numeric_variables<-numeric_variables[,highCorr] # Dropping ASF and 'MR Delay' because of Multicollinearity
head(numeric_variables)
```

# 2.2 Analysis
The original dataset was split into a testing ("alzheimer") and validation ("validation") dataset with a ratio of .9 to .1. We decided to only use 10% of the data for our validation dataset as we only have access to a limited amount of data and wanted to focus on building the moste effective model. In the further analysis we also decided to compare our models in regard to the RMSE to identify the model that reduces both Type I and Type II errors. In all the previous analysis we also only utilized the "alzheimer" dataset so that we do not make any use of information from the validation dataset unconsciously. 

## 2.2.1 Predicting with the Average
To get a baseline comparison point for further analyses we predicted dementia (demented or nondemented) with the average of the variable cCDR from the training dataset (alzheimer). This is the resulting RMSE:

```{r Predicting with the Average, echo = F, fig.align='center', comment = "", message = F, warning=F}
######### Just the Average ##########

# Predicting cCDR of the Validation set with the mean cCDR of the training (alzheimer) dataset
averagecCDR <- mean(as.numeric(as.character(alzheimer$cCDR)))
# RMSE of this prediction by the average
RMSE_Average <- sqrt(mean((as.numeric(as.character(validation$cCDR)) - averagecCDR)^2))
RMSE_Average
rm(averagecCDR) # Remove Average of cCDR
```

We can see that such a RMSE is expected as the prediction relies on the mean point in the "alzheimer" dataset. This only allows a RMSE of around .5 in the validation dataset. 

## 2.2.2 Predicting with a Theory-Based Logistic Regression Model
Recent research into finding protective and risk factors for developing dementia have found that especially the variables: Education, Age, Sex, SES and whole brain volume are pivotal in addressing who is at risk (Jacobsen, 2011). To assess how such a model would do with our dataset we tried to predict dementia (cCDR) with these theory-chosen predictors. Following you can see the RMSE of this theory-based logistic regression model applied to the validation dataset:

```{r Predicting with a theory-based logistic regression model, echo = F, fig.align='center', comment = "", message = F, warning=F}
######### Simple Logistic Regression Model ##########
# Calculating a first simple logistic regression model
model <- glm(cCDR ~ Sex + Age + EDUC + SES + nWBV, family = "binomial", data = alzheimer)
plot.dat <- predict(model, validation)
# RMSE of this theory based simple logistic regression model
RMSE_Logistic <- sqrt(mean(as.numeric(as.character(validation$cCDR)) - exp(plot.dat)/(1+exp(plot.dat)))^2)
RMSE_Logistic
```

As expected, the application of this model results in a much better estimation of which participant is demented and wich participants are not. Following, you can see a visual representation of the effectiveness of this logistic regeression model with "1", blue indicating a person who is demented and "0", red indicating a person who is nondemented. 

```{r Plotting theory-based logistic regression model, echo = F, fig.align='center', comment = "", message = F, warning=F}
# Plotting a Graph for our Model
  # Creating a new dataframe containing the probabilities of having Dementia along with the actual dementia status
predicted.data <- data.frame(
  probability.of.cCDR=model$fitted.values,
  cCDR=alzheimer$cCDR)

  # Sort dataframe from low to high probabilities
predicted.data <- predicted.data[
  order(predicted.data$probability.of.cCDR, decreasing=F),]
  # Add a new column to the dataframe that has the rank of each sample, from low to high probability
predicted.data$rank <- 1:nrow(predicted.data)
  # Actually plotting the graph 
ggplot(data=predicted.data, aes(x=rank, y=probability.of.cCDR)) +
  geom_point(aes(color=cCDR), alpha=1, shape=4, stroke=2) + 
  xlab("Index") + 
  ylab("Predicted probability of getting Dementia")
```

## 2.2.3 Predicting with KNN Model
Following we decided to try to predict cCDR with all the predictor variables included in the dataset and to use cross-validation to develop the best KNN model. Direct diagnostic measures like the MMSE and variables identified to be problematic through, e.g., multicollinearity were excluded. The final model was also tuned to fit the best possible amounts of k, which have been chosen by a cross-validation procedure (only using the training dataset). The following analysis shows which k achives the highest accuracy levels within the dataset. As it has been indicated that the best model follows from using only one nearest neighbor we also employed this parameter value into our KNN model.

```{r Plotting graph with best number of ks for KNN model, echo = F, fig.align='center', comment = "", message = F, warning=F}
######### KNN Model ##########
train <- train(cCDR ~ Sex + Age + EDUC + SES + eTIV + nWBV, method = "knn", 
                   data = alzheimer,
                   tuneGrid = data.frame(k = seq(1, 500, 10)))
# Plotting the Accuracy with various values of k
ggplot(train, highlight = TRUE)
```

Following you can see which number of k shows the highest accuracy levels and how much of the participants were identified as either demented (1) or nondemented (0):
```{r Predicting with KNN Model, echo = F, fig.align='center', comment = "", message = F, warning=F}
# Value of k that achieves the highest accuracy levels
train$bestTune

# Classification of the data according th the final model
train$finalModel
```

This is the resulting RMSE from the KNN Model applied to the validation dataset:
```{r RMSE  KNN Model , echo = F, fig.align='center', comment = "", message = F, warning=F}
# RMSE of this KNN model
RMSE_KNN <- sqrt(mean((as.numeric(as.character(validation$cCDR)) - as.numeric(as.character(predict(train, validation, type = "raw")))))^2)
RMSE_KNN
```

## 2.2.4 Predicting with a Ridge Logistic Regression Model
For the ridge logistic regression model and the following lasso logistic regression model we followed the same data inclusion criteria as for the KNN model. In this model we  include a penalty term $\lambda$. This penalty term reduces the effects of the predictor variables if their effects are only marginally significant or are susceptible to multicollinearity. The higher the lambda, the higher the penalty and reduction of parameter values. In the following model, we used a cross-validation procedure (only the training dataset) to find the best value for $\lambda$ which was then automatically picked. Here you can see the resulting RMSE:

```{r Ridge Logistic Regression Model , echo = F, fig.align='center', comment = "", message = F, warning=F}
######### Ridge & Lasso Logistic Regression Model ##########

# Data Wrangling necessary to use cv.glmnet
## Creating data.matrix with all independent variables from the training dataset (alzheimer)
x.train <- alzheimer %>% select(Sex, Age, EDUC, SES, eTIV, nWBV) %>% data.matrix()
## Creating data.matrix with the dependent variable from the training dataset (alzheimer)
y.train <- alzheimer$cCDR %>% data.matrix()
## Creating data.matrix with the independent variables from the validation dataset (validation)
x.validation <- validation %>% select(Sex, Age, EDUC, SES, eTIV, nWBV) %>% data.matrix()
## Creating data.matrix with the dependent variable from the validation dataset (validation)
y.validation <- validation$cCDR %>% data.matrix()



######### Ridge Logistic Regression Model ##########

# Creating the Ridge Logistic Regression Model
alpha0.fit <- cv.glmnet(x.train, y.train, type.measure="deviance",
                       alpha=0, family="binomial")

# Predicting values for the validation dataset
alpha0.predicted <- 
  predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.validation)

# RMSE Ridge Logistic Regression Model with validation dataset
RMSE_Ridge <- sqrt(mean(as.numeric(as.character(validation$cCDR)) - exp(alpha0.predicted)/(1+exp(alpha0.predicted)))^2)
RMSE_Ridge
```

## 2.2.5 Predicting with a Lasso Logistic Regression Model
Lasso logistic regression is very similar to ridge logistic regression. Also lasso regression utilizes a penalty value of $\lambda$ reducing the effect of unsuitable predictors (described above). In lasso regression models, however, the terms can be penalized until they are reduced to absolute 0. This method called "feature selection" makes the utilized models more parsimonious while also keeping RMSE levels low. In the following code chunck, you can see the RMSE value resulting from lasso logistic regression:


```{r Lasso Logistic Regression Model , echo = F, fig.align='center', comment = "", message = F, warning=F}
######### Lasso Logistic Regression Model ##########

# Creating the Lasso Logistic Regression Model
alpha1.fit <- cv.glmnet(x.train, y.train, type.measure="deviance",
                        alpha=1, family="binomial")

# Predicting values for the validation dataset
alpha1.predicted <- predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=x.validation)

# RMSE Lasso Logistic Regression Model with validation dataset
RMSE_Lasso <- sqrt(mean(as.numeric(as.character(validation$cCDR)) - exp(alpha1.predicted)/(1+exp(alpha1.predicted)))^2)
RMSE_Lasso
```

# 3. Results
This paper set out with the aim of finding a model providing the best RMSE for predicting dementia. We employed five models and and calculated the RMSE values on the validation dataset. In the following table, you can see an overview of these results:

```{r , echo = F, message = F}
#########################################################
# Summary of the Results
#########################################################

# Overview of the reported RMSEs
final <- data.frame(RMSE_Average, RMSE_Logistic, RMSE_KNN, RMSE_Ridge, RMSE_Lasso)

summary <- data.frame(Model = c("Mean", "Logistic", "KNN", "Ridge", "Lasso"),
                 RMSE = c(final$RMSE_Average, final$RMSE_Logistic, final$RMSE_KNN, final$RMSE_Ridge, final$RMSE_Lasso))

kable(summary)
```

As expected, the average was not able to proviede a good prediction for our validation set. The other, more advanced models were able to be much more effective. The model with the lowest RMSE value is the KNN Model with a RMSE value of .026. It is somewhat surprising that the theory based logistic regression model provided estimates comparable to the machine learning models (RMSE = .061). The lasso & ridge logistic regression models were also quite effective, yet, did not provide the lowest RMSE values. In conclusion, the KNN model seems to be very effective in predicting dementia considering a RMSE of .026 and the range of 1 form 1, "demented" to 2,"nondemented."

# 4. Conclusion
This paper supports evidence from previous observations showing that sex, age, education, socioeconomic status, and brain-correlates play an important protective role for dementia  (Nourhashemi et al., 2000). The existing accounts on  protection factors for dementia often fail to utilize models that go beyond the theory driven environment. Although, this allows for narrow theory testing, it is important to discover the interplay between prediction/protective factors. The results of this paper might aid in developing a data science driven programm that is able to detect dementia in a given clinical population. Applying the practical approaches discussed in the present  paper to heritable forms of dementia, such machine learning algorithms might aid in fast and reliable diagnoses. 


# 5. References
Chen, R., Hu, Z., Wei, L., & Wilson, K. (2014). Socioeconomic status and survival among older adults with dementia and depression. The British Journal of Psychiatry, 204(6), 436–440.

Cohen, C. I. (1994). Education, occupation, and alzheimer's disease. Jama: The Journal of the American Medical Association, 272(18), 1405–1405. https://doi.org/10.1001/jama.272.18.1405c

Jacobsen, S. R. (Ed.). (2011). Vascular dementia : risk factors, diagnosis, and treatment (Ser. Neuroscience research progress). Nova Science Publishers.

Kishore, P., Usha Kumari, C., Kumar, M. N. V. S. S., & Pavani, T. (2020). Detection and analysis of alzheimer’s disease using various machine learning algorithms. Materials Today: Proceedings. https://doi.org/10.1016/j.matpr.2020.07.645

Nourhashemi, F., Gillette-Guyonnet, S., Andrieu, S., Ghisolfi, A., Ousset, P. J., Grandjean, H., … Albarede, J. L. (2000). Alzheimer disease: protective factors. The American Journal of Clinical Nutrition, 71(2), 643.

Prince, M. (2007). Epidemiology of dementia. Psychiatry, 6(12), 488–490. https://doi.org/10.1016/j.mppsy.2007.10.001

Scuteri, D., Rombola, L., Morrone, L. A., Bagetta, G., Sakurada, S., Sakurada, T., … Corasaniti, M. T. (2019). Neuropharmacology of the neuropsychiatric symptoms of dementia and role of pain: essential oil of bergamot as a novel therapeutic approach. International Journal of Molecular Sciences, 20(13). https://doi.org/10.3390/ijms20133327

Whalley, L. J., & Breitner, J. C. S. (2009). Dementia (2nd ed., Ser. Fast facts). HEALTH Press.